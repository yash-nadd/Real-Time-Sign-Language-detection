{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q2b6d3FAV12",
        "outputId": "772b7ba9-b53c-405b-dcdf-3891341b73d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/Dataset.zip\"  # Path to the copied ZIP file\n",
        "extract_path = \"/content/extracted\"  # Destination folder\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extraction completed. Files are in:\", extract_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NzAdmBIAdEV",
        "outputId": "a348f8d6-5b64-43c4-a19f-0ed0e886fb47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed. Files are in: /content/extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import csv\n",
        "import copy\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "# Initialize Mediapipe Hand Module\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "# Function to extract hand landmarks (x, y, z)\n",
        "def calc_landmark_list(image, landmarks):\n",
        "    image_width, image_height = image.shape[1], image.shape[0]\n",
        "    return [\n",
        "        [min(int(l.x * image_width), image_width - 1),\n",
        "         min(int(l.y * image_height), image_height - 1),\n",
        "         l.z]  # Depth (relative to wrist)\n",
        "        for l in landmarks.landmark\n",
        "    ]\n",
        "\n",
        "# Function to normalize landmarks (x, y, z)\n",
        "def pre_process_landmark(landmark_list):\n",
        "    base_x, base_y, base_z = landmark_list[0]  # Use wrist as reference\n",
        "    temp_landmark_list = [[x - base_x, y - base_y, z - base_z] for x, y, z in landmark_list]\n",
        "\n",
        "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))  # Flatten\n",
        "    max_value = max(map(abs, temp_landmark_list), default=1)  # Normalize\n",
        "    return [n / max_value for n in temp_landmark_list]\n",
        "\n",
        "# Function to log data into CSV\n",
        "def logging_csv(letter, landmark_list):\n",
        "    csv_path = '/content/keypointAug.csv'  # Save CSV in Colab workspace\n",
        "    with open(csv_path, 'a', newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([letter, *landmark_list])  # Save label + features\n",
        "\n",
        "# **Define dataset path**\n",
        "dataset_path = '/content/extracted/Dataset'\n",
        "\n",
        "# **Dynamically get all class labels (folder names)**\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"❌ Dataset directory not found: {dataset_path}\")\n",
        "    exit()\n",
        "\n",
        "labels = sorted([f for f in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, f))])\n",
        "print(f\"📂 Found {len(labels)} classes: {labels}\")\n",
        "\n",
        "# Initialize Mediapipe Hands Model\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.5) as hands:\n",
        "\n",
        "    # **Loop through each folder (class label)**\n",
        "    for label in labels:\n",
        "        folder_path = os.path.join(dataset_path, label)\n",
        "        image_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "        print(f\"🔄 Processing class: {label} ({len(image_files)} images)\")\n",
        "\n",
        "        # **Loop through each image**\n",
        "        for file in image_files:\n",
        "            if not os.path.exists(file):\n",
        "                print(f\"❌ File not found: {file}\")\n",
        "                continue\n",
        "\n",
        "            # **Read Image**\n",
        "            image = cv2.imread(file)\n",
        "            if image is None:\n",
        "                print(f\"⚠️ Corrupt image: {file}\")\n",
        "                continue\n",
        "\n",
        "            image = cv2.flip(image, 1)  # Flip for consistency\n",
        "            results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "            if not results.multi_hand_landmarks:\n",
        "                print(f\"🚫 No hand detected in {file}\")\n",
        "                continue\n",
        "\n",
        "            # **Process landmarks**\n",
        "            for hand_landmarks in results.multi_hand_landmarks:\n",
        "                landmark_list = calc_landmark_list(image, hand_landmarks)\n",
        "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
        "\n",
        "                # **Debug output**\n",
        "                print(f\"✅ Processed: {file} -> Label: {label}\")\n",
        "\n",
        "                # **Log to CSV**\n",
        "                logging_csv(label, pre_processed_landmark_list)\n",
        "\n",
        "print(\"✅ Data extraction complete! CSV saved at /content/keypoint.csv\")\n"
      ],
      "metadata": {
        "id": "Xk_tTQGZEol7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dybXVRVvqmh2"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}